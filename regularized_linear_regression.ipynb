{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102,)\n",
      "iteration  0\n",
      "total loss 276.42467821782174\n",
      "total loss 276.42467821782174\n",
      "iteration  1\n",
      "total loss 228.41164636028537\n",
      "total loss 228.41164636028537\n",
      "iteration  2\n",
      "total loss 190.06663800318643\n",
      "total loss 190.06663800318643\n",
      "iteration  3\n",
      "total loss 159.37310726763312\n",
      "total loss 159.37310726763312\n",
      "iteration  4\n",
      "total loss 134.7566161466007\n",
      "total loss 134.7566161466007\n",
      "iteration  5\n",
      "total loss 114.9814974013094\n",
      "total loss 114.9814974013094\n",
      "iteration  6\n",
      "total loss 99.07349111124971\n",
      "total loss 99.07349111124971\n",
      "iteration  7\n",
      "total loss 86.26134064960232\n",
      "total loss 86.26134064960232\n",
      "iteration  8\n",
      "total loss 75.93234971708648\n",
      "total loss 75.93234971708648\n",
      "iteration  9\n",
      "total loss 67.59830903333837\n",
      "total loss 67.59830903333837\n",
      "iteration  10\n",
      "total loss 60.8691913165802\n",
      "total loss 60.8691913165802\n",
      "iteration  11\n",
      "total loss 55.432714283917036\n",
      "total loss 55.432714283917036\n",
      "iteration  12\n",
      "total loss 51.03837151301902\n",
      "total loss 51.03837151301902\n",
      "iteration  13\n",
      "total loss 47.48489052278655\n",
      "total loss 47.48489052278655\n",
      "iteration  14\n",
      "total loss 44.61033800197115\n",
      "total loss 44.61033800197115\n",
      "iteration  15\n",
      "total loss 42.28428258022294\n",
      "total loss 42.28428258022294\n",
      "iteration  16\n",
      "total loss 40.40156596564813\n",
      "total loss 40.40156596564813\n",
      "iteration  17\n",
      "total loss 38.87733771144931\n",
      "total loss 38.87733771144931\n",
      "iteration  18\n",
      "total loss 37.643087210764705\n",
      "total loss 37.643087210764705\n",
      "iteration  19\n",
      "total loss 36.64346576388008\n",
      "total loss 36.64346576388008\n",
      "iteration  20\n",
      "total loss 35.833736721878076\n",
      "total loss 35.833736721878076\n",
      "iteration  21\n",
      "total loss 35.17772638874973\n",
      "total loss 35.17772638874973\n",
      "iteration  22\n",
      "total loss 34.64617517483083\n",
      "total loss 34.64617517483083\n",
      "iteration  23\n",
      "total loss 34.2154093518917\n",
      "total loss 34.2154093518917\n",
      "iteration  24\n",
      "total loss 33.86627007712339\n",
      "total loss 33.86627007712339\n",
      "iteration  25\n",
      "total loss 33.583249181693404\n",
      "total loss 33.583249181693404\n",
      "iteration  26\n",
      "total loss 33.35379134960127\n",
      "total loss 33.35379134960127\n",
      "iteration  27\n",
      "total loss 33.1677303425145\n",
      "total loss 33.1677303425145\n",
      "iteration  28\n",
      "total loss 33.016833312652984\n",
      "total loss 33.016833312652984\n",
      "iteration  29\n",
      "total loss 32.894432339559025\n",
      "total loss 32.894432339559025\n",
      "iteration  30\n",
      "total loss 32.79512639932817\n",
      "total loss 32.79512639932817\n",
      "iteration  31\n",
      "total loss 32.714540238047526\n",
      "total loss 32.714540238047526\n",
      "iteration  32\n",
      "total loss 32.649129240349794\n",
      "total loss 32.649129240349794\n",
      "iteration  33\n",
      "total loss 32.59602148939414\n",
      "total loss 32.59602148939414\n",
      "iteration  34\n",
      "total loss 32.55288990914348\n",
      "total loss 32.55288990914348\n",
      "iteration  35\n",
      "total loss 32.5178487451434\n",
      "total loss 32.5178487451434\n",
      "iteration  36\n",
      "total loss 32.48936974106394\n",
      "total loss 32.48936974106394\n",
      "iteration  37\n",
      "total loss 32.4662142568626\n",
      "total loss 32.4662142568626\n",
      "iteration  38\n",
      "total loss 32.44737829200743\n",
      "total loss 32.44737829200743\n",
      "iteration  39\n",
      "total loss 32.432047956982935\n",
      "total loss 32.432047956982935\n",
      "iteration  40\n",
      "total loss 32.419563404951916\n",
      "total loss 32.419563404951916\n",
      "iteration  41\n",
      "total loss 32.40938961440567\n",
      "total loss 32.40938961440567\n",
      "iteration  42\n",
      "total loss 32.401092720161756\n",
      "total loss 32.401092720161756\n",
      "iteration  43\n",
      "total loss 32.394320838069596\n",
      "total loss 32.394320838069596\n",
      "iteration  44\n",
      "total loss 32.38878852947802\n",
      "total loss 32.38878852947802\n",
      "iteration  45\n",
      "total loss 32.38426421395751\n",
      "total loss 32.38426421395751\n",
      "iteration  46\n",
      "total loss 32.38055997026564\n",
      "total loss 32.38055997026564\n",
      "iteration  47\n",
      "total loss 32.37752327200424\n",
      "total loss 32.37752327200424\n",
      "iteration  48\n",
      "total loss 32.37503029061749\n",
      "total loss 32.37503029061749\n",
      "iteration  49\n",
      "total loss 32.37298046818195\n",
      "total loss 32.37298046818195\n",
      "iteration  50\n",
      "total loss 32.37129211896819\n",
      "total loss 32.37129211896819\n",
      "iteration  51\n",
      "total loss 32.36989886453376\n",
      "total loss 32.36989886453376\n",
      "iteration  52\n",
      "total loss 32.36874674418735\n",
      "total loss 32.36874674418735\n",
      "iteration  53\n",
      "total loss 32.367791872696\n",
      "total loss 32.367791872696\n",
      "iteration  54\n",
      "total loss 32.366998541434434\n",
      "total loss 32.366998541434434\n",
      "iteration  55\n",
      "total loss 32.36633767888047\n",
      "total loss 32.36633767888047\n",
      "iteration  56\n",
      "total loss 32.365785602322994\n",
      "total loss 32.365785602322994\n",
      "iteration  57\n",
      "total loss 32.365323005579555\n",
      "total loss 32.365323005579555\n",
      "iteration  58\n",
      "total loss 32.36493413799633\n",
      "total loss 32.36493413799633\n",
      "iteration  59\n",
      "total loss 32.36460613848893\n",
      "total loss 32.36460613848893\n",
      "iteration  60\n",
      "total loss 32.36432849525823\n",
      "total loss 32.36432849525823\n",
      "iteration  61\n",
      "total loss 32.36409260738491\n",
      "total loss 32.36409260738491\n",
      "iteration  62\n",
      "total loss 32.36389142901951\n",
      "total loss 32.36389142901951\n",
      "iteration  63\n",
      "total loss 32.36371918054075\n",
      "total loss 32.36371918054075\n",
      "iteration  64\n",
      "total loss 32.36357111401755\n",
      "total loss 32.36357111401755\n",
      "iteration  65\n",
      "total loss 32.363443322710296\n",
      "total loss 32.363443322710296\n",
      "iteration  66\n",
      "total loss 32.36333258629172\n",
      "total loss 32.36333258629172\n",
      "iteration  67\n",
      "total loss 32.36323624504406\n",
      "total loss 32.36323624504406\n",
      "iteration  68\n",
      "total loss 32.36315209756571\n",
      "total loss 32.36315209756571\n",
      "iteration  69\n",
      "total loss 32.36307831755594\n",
      "total loss 32.36307831755594\n",
      "iteration  70\n",
      "total loss 32.36301338608438\n",
      "total loss 32.36301338608438\n",
      "iteration  71\n",
      "total loss 32.36295603643216\n",
      "total loss 32.36295603643216\n",
      "iteration  72\n",
      "total loss 32.36290520914188\n",
      "total loss 32.36290520914188\n",
      "iteration  73\n",
      "total loss 32.36286001536051\n",
      "total loss 32.36286001536051\n",
      "iteration  74\n",
      "total loss 32.362819706920924\n",
      "total loss 32.362819706920924\n",
      "iteration  75\n",
      "total loss 32.362783651901125\n",
      "total loss 32.362783651901125\n",
      "iteration  76\n",
      "total loss 32.362751314638146\n",
      "total loss 32.362751314638146\n",
      "iteration  77\n",
      "total loss 32.3627222393664\n",
      "total loss 32.3627222393664\n",
      "iteration  78\n",
      "total loss 32.362696036806234\n",
      "total loss 32.362696036806234\n",
      "iteration  79\n",
      "total loss 32.36267237315583\n",
      "total loss 32.36267237315583\n",
      "iteration  80\n",
      "total loss 32.36265096104174\n",
      "total loss 32.36265096104174\n",
      "iteration  81\n",
      "total loss 32.362631552067114\n",
      "total loss 32.362631552067114\n",
      "iteration  82\n",
      "total loss 32.36261393066427\n",
      "total loss 32.36261393066427\n",
      "iteration  83\n",
      "total loss 32.36259790901287\n",
      "total loss 32.36259790901287\n",
      "iteration  84\n",
      "total loss 32.3625833228299\n",
      "total loss 32.3625833228299\n",
      "iteration  85\n",
      "total loss 32.36257002787346\n",
      "total loss 32.36257002787346\n",
      "iteration  86\n",
      "total loss 32.36255789703181\n",
      "total loss 32.36255789703181\n",
      "iteration  87\n",
      "total loss 32.362546817893005\n",
      "total loss 32.362546817893005\n",
      "iteration  88\n",
      "total loss 32.36253669070972\n",
      "total loss 32.36253669070972\n",
      "iteration  89\n",
      "total loss 32.36252742668967\n",
      "total loss 32.36252742668967\n",
      "iteration  90\n",
      "total loss 32.36251894655469\n",
      "total loss 32.36251894655469\n",
      "iteration  91\n",
      "total loss 32.362511179322006\n",
      "total loss 32.362511179322006\n",
      "iteration  92\n",
      "total loss 32.362504061269725\n",
      "total loss 32.362504061269725\n",
      "iteration  93\n",
      "total loss 32.362497535055326\n",
      "total loss 32.362497535055326\n",
      "iteration  94\n",
      "total loss 32.362491548961664\n",
      "total loss 32.362491548961664\n",
      "iteration  95\n",
      "total loss 32.362486056249416\n",
      "total loss 32.362486056249416\n",
      "iteration  96\n",
      "total loss 32.362481014598856\n",
      "total loss 32.362481014598856\n",
      "iteration  97\n",
      "total loss 32.36247638562653\n",
      "total loss 32.36247638562653\n",
      "iteration  98\n",
      "total loss 32.36247213446528\n",
      "total loss 32.36247213446528\n",
      "iteration  99\n",
      "total loss 32.36246822939778\n",
      "total loss 32.36246822939778\n",
      "optimal values of weight is [[-2.41881986  0.92977086 -2.97373174]]\n",
      "optimal values of bias is 22.503884257125613\n",
      " total loss 19.949437726448384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing and splitting training test sets\n",
    "data = load_boston()\n",
    "Y=data.target\n",
    "#X=data.data\n",
    "X=np.array(data.data)\n",
    "X=X[:,:3]\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)\n",
    "mean = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0)\n",
    "X_train = (X_train - mean) / sigma\n",
    "N=len(X_train)\n",
    "weights=np.zeros((3,1))\n",
    "b=1\n",
    "\n",
    "value_X=np.transpose(X_train)\n",
    "value_Y=y_train\n",
    "value_weights=np.transpose(weights)\n",
    "\n",
    "test_X=X_test\n",
    "mean1=test_X.mean(axis=0)\n",
    "sigma1=test_X.std(axis=0)\n",
    "final_test_X=(np.transpose((test_X - mean1) / sigma1)).T\n",
    "test_Y=y_test\n",
    "print(test_Y.shape)\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "#lyambda value\n",
    "lda= 0.3\n",
    "\n",
    "#print(len(X.T))\n",
    "def error(X,Y,W,b):\n",
    "    error_matrix=((((np.matmul(W,X))+b)-Y))\n",
    "    #print(np.shape(error_matrix))\n",
    "    new_value=np.square(error_matrix)\n",
    "    total_loss=(1/2) * np.sum(new_value)\n",
    "    final_total_loss=total_loss/len(Y)\n",
    "    print(\"total loss\",final_total_loss)\n",
    "    \n",
    "    return error_matrix\n",
    "#     print(error_matrix.shape)\n",
    "    #left to find total cost(ie total error)\n",
    "    \n",
    "    \n",
    "def gradient_step(X,Y,W,b,learning_rate,i):\n",
    "    X_transpose=np.transpose(X)    \n",
    "    if(i==1):\n",
    "        gradient_weights=(1/ len(Y)) * (np.matmul(error(X,Y,W,b),X_transpose))\n",
    "        gradient_bias=(1/len(Y)) * np.sum(error(X,Y,W,b))\n",
    "        new_w = W - learning_rate*gradient_weights\n",
    "        new_b = b - learning_rate*gradient_bias\n",
    "        return [new_w,new_b]\n",
    "    else:\n",
    "        gradient_weights=(1/ len(Y)) * (np.matmul(error(X,Y,W,b),X_transpose)+(lda/len(Y))*np.sum(W**2))\n",
    "        #pint(\"Gradient: \", gradient_weights.shape)\n",
    "        gradient_bias=(1/len(Y)) * np.sum(error(X,Y,W,b))\n",
    "        new_w = W - learning_rate*gradient_weights\n",
    "        new_b = b - learning_rate*gradient_bias\n",
    "        return [new_w,new_b]        \n",
    "       \n",
    "def gradient_descent_runner(X,Y,W,b,learning_rate,epochs):\n",
    "    final_w=W\n",
    "    final_b=b\n",
    "    for i in range(epochs):\n",
    "        print(\"iteration \",i)\n",
    "        [final_w,final_b] = gradient_step(X,Y,final_w,final_b,learning_rate,i)\n",
    "    print(\"optimal values of weight is\",final_w)\n",
    "    print(\"optimal values of bias is\",final_b)\n",
    "    return final_w,final_b\n",
    "\n",
    "def test_error(X,Y,W,b):\n",
    "        error_matrix=((((np.matmul(W,X.T))+b)-Y))\n",
    "        new_value=np.square(error_matrix)\n",
    "        total_loss=(1/2) * np.sum(new_value)\n",
    "        final_total_loss=total_loss/len(Y)\n",
    "        print(\" total loss\",final_total_loss)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "# for training data    \n",
    "optimal_w,optimal_b=gradient_descent_runner(value_X,value_Y,value_weights,b,learning_rate,epochs)\n",
    "#for test case\n",
    "test_error(final_test_X,test_Y,optimal_w,optimal_b)\n",
    "\n",
    "\n",
    "\n",
    "#gradient_descent_runner(final_test_X,value_Y,value_weights,b,learning_rate,epochs)\n",
    "# def test_case:\n",
    "    \n",
    "#     optimal_weight=[[-0.91021802  1.1260376  -0.18719025  0.59404272 -2.09999691  2.80006782\n",
    "#        0.05557707 -3.25971239  2.58286091 -2.12193079 -1.99438135  0.91845471\n",
    "#       -3.30065244]]\n",
    "#     optimal_bias=22.449257425742616\n",
    "#     plot.plot()\n",
    "    \n",
    "# weight=[[-2.07162692,1.37691449,-2.9149935 ]]\n",
    "# final_weight=np.transpose(weight)\n",
    "# print(final_weight.shape)\n",
    "# final_bias=22.577970297029694  \n",
    "# prediction=np.matmul(weight,X_test.T)+final_bias\n",
    "# final_X_test=np.transpose(X_test)\n",
    "# error(final_X_test,y_test,weight,final_bias)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "                         \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
